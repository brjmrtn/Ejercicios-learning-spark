{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b361f384-1e78-4afa-af24-13932f9b483b",
   "metadata": {},
   "source": [
    "### UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e17bfaa-2e8a-4ef0-b8ba-ca7e678f8f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d1de13d-d43d-4d58-87a9-4257a881cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cubed functions\n",
    "def cubed(s):\n",
    "    return s*s*s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a319377-7e6a-4584-a974-6fc96d0e9caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('Capitulo5')\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5e25275-8300-43e8-b845-48f75426efd2",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Register UDF\n",
    "spark.udf.register('cubed', cubed, LongType())\n",
    "\n",
    "# Generte temprary view\n",
    "spark.range(1,9).createOrReplaceTempView('udf_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf2cc4-6c5a-4338-948f-e2e8fba20f4e",
   "metadata": {},
   "source": [
    "**Spark SQL** para ejecutar cubed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfc37dc-9af5-4fb6-92a3-7ba37192290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT id, cubed(id) AS id_cubed FROM udf_test').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a04e2a2",
   "metadata": {},
   "source": [
    "#### Evaluation order cheking in Spark Sql\n",
    "\n",
    "* Spark SQL no garantiza el orden de la evaluaciÃ³n de las subexpresiones. En este ejemplo no garantiza que la clausula `IS NOT NULL` se ejecute antes que `strlen(s) > 1` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111183df",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql ('SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) > 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a43bc8-41dd-4b97-91fa-47d555d6f4aa",
   "metadata": {},
   "source": [
    "#### UDF Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dbfaf18-38be-40d7-9591-b4e12ec524ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install PyArrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccaef9db-a4cb-4bbf-a3f2-fa6a5b36814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import various pysparkSQL functions including pandas_udf\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "# declared the cubed function\n",
    "def cubed(a: pd.Series) -> pd.Series:\n",
    "    return a * a * a\n",
    "\n",
    "# Create the pandas UDF for the cubed function\n",
    "\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b30079-5c17-4b6f-9a57-ec59faea452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "x = pd.Series([1, 2, 3]) # Create a pd Series\n",
    "\n",
    "# The function for a pandas_udf executed with local Pandas data\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b6cac1-0bdc-4cf5-9590-6a2c7106462b",
   "metadata": {},
   "source": [
    "Cambiar a DF de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9689aac-f153-4a5e-a778-bdf668b8f631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|cubed(id)|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFRame, 'spark' is an existing SparkSession\n",
    "df = spark.range(1, 4)\n",
    "\n",
    "# Execute functions as a Spark vectorized UDF\n",
    "df.select('id', cubed_udf(col('id'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538125f4",
   "metadata": {},
   "source": [
    "### PostgreSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa950cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read option 1: Loading data from a JDBC source using load method\n",
    "jdbcDF1 = (spark.read.format(\"jdbc\")\n",
    "          .option('url', 'jdbc:postgresql://[DBSERVER]')\n",
    "          .option('dbtable', '[SCHEMA].[TABLENAME]')\n",
    "          .option('user', '[USERNAME]')\n",
    "          .option('password', '[PASSWORD]')\n",
    "          .load())\n",
    "\n",
    "# Read Option 2: Loading data from JDBC source using jdbc method\n",
    "jdbcDF2 = (spark.read.jdbc(\"jdbc:postgresql://[DBSERVER]\", \"[SCHEMA].[TABLENAME]\",\n",
    "                          properties={\"user\":\"[USERNAME]\", \"password\": \"[PASSWORD]\"}))\n",
    "\n",
    "# Write Option 1: Saving data to a JDBC source using save method\n",
    "(jdbcDF1.write\n",
    "    .format('jdbc')\n",
    "    .option('url', 'jdbc:postgresql://[DBSERVER]')\n",
    "    .option('dbtable', '[SCHEMA].[TABLENAME]')\n",
    "    .option('user', '[USERNAME]')\n",
    "    .option('password', '[PASSWORD]')\n",
    "    .save())\n",
    "\n",
    "# Write Option 2: Saving data to a JDBC source using jdbc method\n",
    "(jdbcDF2.write.jdbc('jdbc:postgresql:[DEBESERVER]', '[SCHEMA].[TABLENAME]',\n",
    "                   properties={'user': '{USERNAME}', 'password': '[PASSWORD]'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadb6f2",
   "metadata": {},
   "source": [
    "### MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89878b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------+---------+------+----------+\n",
      "|emp_no|birth_date|first_name|last_name|gender| hire_date|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26|\n",
      "| 10002|1964-06-02|   Bezalel|   Simmel|     F|1985-11-21|\n",
      "| 10003|1959-12-03|     Parto|  Bamford|     M|1986-08-28|\n",
      "| 10004|1954-05-01| Chirstian|  Koblick|     M|1986-12-01|\n",
      "| 10005|1955-01-21|   Kyoichi| Maliniak|     M|1989-09-12|\n",
      "| 10006|1953-04-20|    Anneke|  Preusig|     F|1989-06-02|\n",
      "| 10007|1957-05-23|   Tzvetan|Zielinski|     F|1989-02-10|\n",
      "| 10008|1958-02-19|    Saniya| Kalloufi|     M|1994-09-15|\n",
      "| 10009|1952-04-19|    Sumant|     Peac|     F|1985-02-18|\n",
      "| 10010|1963-06-01| Duangkaew| Piveteau|     F|1989-08-24|\n",
      "+------+----------+----------+---------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "passKey = '************'\n",
    "\n",
    "spark = SparkSession.builder.appName('Capitulo05').getOrCreate()\n",
    "employee_df = (spark.read.format('jdbc')\n",
    "            .option('url', 'jdbc:mysql://localhost:3306/employees')\n",
    "            .option('driver', 'com.mysql.cj.jdbc.Driver')\n",
    "            .option('dbtable', 'employees')\n",
    "            .option('user', 'root')\n",
    "            .option('password', passKey)\n",
    "            .load())\n",
    "\n",
    "employee_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d80f21a",
   "metadata": {},
   "source": [
    "#### HIGHER-ORDER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5130a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField('celsius', ArrayType(IntegerType()))])\n",
    "\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]]\n",
    "\n",
    "t_c = spark.createDataFrame(t_list, schema)\n",
    "t_c.createOrReplaceTempView('tC')\n",
    "# Show the DF\n",
    "t_c.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a3f943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|           Farenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Transform()\n",
    "# Calculate Farenheit from Celsius for an array of temperatures\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "transform(celsius, t -> ((t*9) div 5) + 32) as Farenheit\n",
    "FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06eed0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter()--------- Filter temperatures > 38C for array temperatures\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius, filter(celsius, t -> t > 38) as high\n",
    "FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcf1405a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# exists()----- Is there a temperature of 38C in the array of temperature\n",
    "spark.sql(\"\"\"\n",
    "SELECT celsius,\n",
    "exists(celsius, t -> t = 38) as threshold\n",
    "FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81141f13",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-8ca6e8eba974>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# reduce()---- Calculate average temperature and convert to F\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m spark.sql(\"\"\"SELECT celsius,\n\u001b[0m\u001b[0;32m      3\u001b[0m reduce(celsius,\n\u001b[0;32m      4\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \"\"\"\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Undefined function: 'reduce'. This function is neither a registered temporary function nor a permanent function registered in the database 'default'.; line 2 pos 0"
     ]
    }
   ],
   "source": [
    "# reduce()---- Calculate average temperature and convert to F\n",
    "spark.sql(\"\"\"SELECT celsius,\n",
    "reduce(celsius,\n",
    "0,\n",
    "(t, acc) -> t + acc,\n",
    "acc -> (acc div size(celsius) * 9 div 5) + 32\n",
    ") as avgFarenheit FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f6fdd0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files path\n",
    "tripdelaysFilePath = 'data/departuredelays.csv'\n",
    "airportsnaFilePath = 'data/airportCodesNa.txt'\n",
    "\n",
    "# Obtain airports data set\n",
    "airportsna = (spark.read\n",
    "                .format(\"csv\")\n",
    "                .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    "                .load(airportsnaFilePath))\n",
    "\n",
    "\n",
    "airportsna.createOrReplaceTempView('airport_na')\n",
    "\n",
    "# Obtain departure delays data set\n",
    "departureDelays =(spark.read\n",
    "                 .format('csv')\n",
    "                 .options(header='true')\n",
    "                 .load(tripdelaysFilePath))\n",
    "\n",
    "departureDelays = (departureDelays.withColumn('delay', expr('CAST(delay as INT) as delay'))\n",
    "                  .withColumn('distance', expr('CAST(distance as INT) as distance')))\n",
    "\n",
    "departureDelays.createOrReplaceTempView('departureDelays')\n",
    "\n",
    "# Create temporary small table\n",
    "foo = (departureDelays\n",
    "      .filter(expr(\"\"\"origin == 'SEA' and destination =='SFO' and\n",
    "      date like '01010%' and delay > 0\"\"\")))\n",
    "foo.createOrReplaceTempView('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "57c19e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airport_na LiMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac60201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2affb693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b9408",
   "metadata": {},
   "source": [
    "### UNIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "68db7b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Union two tables\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView('bar')\n",
    "\n",
    "# Show the union filtering for SEA and SFO in a specific time range\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9e4bae",
   "metadata": {},
   "source": [
    "### JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "309bca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join departure delays data (foo) with airport info\n",
    "foo.join(airportsna,\n",
    "        airportsna.IATA == foo.origin\n",
    "        ).select('City', 'State', 'date', 'delay', 'distance', 'destination').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5a077",
   "metadata": {},
   "source": [
    "### Windowing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74f9f58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
